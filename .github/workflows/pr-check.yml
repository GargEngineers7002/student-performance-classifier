name: PR Submission Check

on:
  pull_request:
    types: [opened, synchronize, reopened]
    branches:
      - main  # Adjust if your default branch is different

jobs:
  evaluate-submission:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Execute student notebook
      run: |
        jupyter nbconvert --to notebook --execute notebooks/student_analysis.ipynb --output notebooks/student_analysis_executed.ipynb
      continue-on-error: true  # Allow to proceed even if notebook fails, for evaluation

    - name: Run evaluation script
      id: eval
      run: python scripts/evaluate_submission.py
      continue-on-error: true

    - name: Comment on PR
      uses: actions/github-script@v7
      with:
        script: |
          const { exec } = require('child_process');
          const { promisify } = require('util');
          const execAsync = promisify(exec);

          try {
            const { stdout, stderr } = await execAsync('python scripts/evaluate_submission.py');
            const output = stdout + stderr;

            // Parse the output
            const lines = output.split('\n');
            let passed = false;
            let points = 0;
            let feedback = [];

            for (const line of lines) {
              if (line.startsWith('Passed: ')) {
                passed = line.split(': ')[1] === 'True';
              } else if (line.startsWith('Points: ')) {
                points = parseInt(line.split(': ')[1]);
              } else if (line.startsWith('Feedback:')) {
                // Next lines are feedback
              } else if (line.startsWith('- ')) {
                feedback.push(line.substring(2));
              }
            }

            const comment = `## Submission Evaluation Results\n\n**Points Awarded:** ${points}/150\n\n**Status:** ${passed ? '✅ Passed' : '❌ Needs Improvement'}\n\n**Feedback:**\n${feedback.map(f => `- ${f}`).join('\n')}\n\n${passed ? 'Great job! Your submission meets all requirements.' : 'Please review the feedback and improve your model.'}`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

            // Add label
            const label = passed ? 'passed' : 'needs-improvement';
            github.rest.issues.addLabels({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              labels: [label]
            });

          } catch (error) {
            console.error('Error running evaluation:', error);
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: '❌ Error evaluating submission. Please check your submission files and try again.'
            });
          }